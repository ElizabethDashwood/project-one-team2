# Dependencies and Setup
import pandas as pd  
import numpy as np


# Specify the path to your Excel file
household_income_data = "Resources/5204055011do001.xlsx"  


# Define the row indices to skip
rows_to_skip = list(range(0, 6)) + [7] + list(range(9, 26)) + list(range(27, 40)) + list(range(42, 127))  


# Read ABS income data file and store into Pandas DataFrames
dfs = pd.read_excel(household_income_data, sheet_name=None, skiprows=rows_to_skip)  


# Create an empty list to store DataFrames with tab names as a column
dfs_with_tab_name = [] 


# Flag to skip the first and last tab
skip_first_tab = True
skip_last_tab = False


# Iterate over each sheet in the dictionary
for sheet_name, df in dfs.items():
    if skip_first_tab:
        skip_first_tab = False
        continue  # Skip processing the first tab
        
    if skip_last_tab:
        break  # Exit the loop if the last tab is reached
        
    # Determine the column indices dynamically based on the actual number of columns
    num_cols = min(7, len(df.columns))
    
    # Ensure we don't exceed the number of columns in the DataFrame
    columns_to_read = list(range(num_cols))
    
    # Select only the specified columns
    df_selected = df.iloc[:, columns_to_read].copy()
    
    # Add a new column with the tab name to the selected DataFrame using loc
    df_selected.loc[:, 'Year/s'] = sheet_name
    
    # Append the modified DataFrame to the list
    dfs_with_tab_name.append(df_selected)     


# Concatenate all DataFrames in the list into a single DataFrame
combined_df = pd.concat(dfs_with_tab_name, ignore_index=True)  


# Define a dictionary to map old values to new values
value_mapping = {'Table 1.1': '2003-2004', 'Table 1.2': '2005-2006', 'Table 1.3': '2007-2008', 'Table 1.4': '2009-2010', 'Table 1.5': '2011-2012', 'Table 1.6': '2013-2014', 'Table 1.7': '2015-2016', 'Table 1.8': '2017-2018', 'Table 1.9': '2019-2020', 'Table 1.10': '2020-2021', 'Table 1.11': '2021-2022', 'Explanatory Notes': '', 'tab_name': 'Year/s'}


# Replace values in column 'tab_name' using the value_mapping dictionary
combined_df['Year/s'] = combined_df['Year/s'].replace(value_mapping)


# Remove rows where column 'Year/s' has value '2020-2021' or '2021-2022'
values_to_remove = ['2020-2021', '2021-2022']
cleaned_df = combined_df[~combined_df['Year/s'].isin(values_to_remove)]


# Convert values to numeric data type
cleaned_df.loc[:, 'Wages and salaries'] = cleaned_df['Wages and salaries'].astype(float)
cleaned_df.loc[:, 'Income from  unincorporated business (a)'] = cleaned_df['Income from  unincorporated business (a)'].astype(float)
cleaned_df.loc[:, 'Property income and superannuation'] = cleaned_df['Property income and superannuation'].astype(float)
cleaned_df.loc[:, 'Government pensions and allowances'] = cleaned_df['Government pensions and allowances'].astype(float)
cleaned_df.loc[:, 'Other'] = cleaned_df['Other'].astype(float)
cleaned_df.loc[:, 'All households (b)'] = cleaned_df['All households (b)'].astype(float)
cleaned_df.loc[:, 'Year/s'] = cleaned_df['Year/s'].astype(str)


# Specify the file path for the CSV file
csv_file_path = 'ABS_income_data.csv'  


# Save the DataFrame to a CSV file
cleaned_df.to_csv(csv_file_path, index=False) 


print(f"DataFrame successfully saved to '{csv_file_path}'.")


# Create a pivot table with 'Category' as rows, 'Year' as columns, and 'Value' as values
pivot_df = cleaned_df.pivot_table(columns='Year/s', values='All households (b)', aggfunc='sum')



# rename the index as "Household income"
pivot_df = cleaned_df.pivot_table(index='Unnamed: 0', columns='Year/s', values='All households (b)', aggfunc='sum')
pivot_df = pivot_df.rename_axis('Household income')


# Define a function to format values as currency with 2 decimal places
def format_currency(value):
    return "${:,.2f}".format(value)


# Apply the currency formatting with 2 decimal places to the pivot table values
formatted_pivot_df = pivot_df.map(format_currency)


# Display the formatted pivot table
formatted_pivot_df
##########NEED TO FIX FORMATTING OF FIRST ROW############################



